{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d440459a-5343-4543-b58c-5fd965e309a2",
   "metadata": {},
   "source": [
    "# Building Models to rationalize SAR\n",
    "\n",
    "So far, we have compiled a dataset containing binding data for various PKA binding molecules. In this second part of the practical, we will try to build simple models that can help us rationalize why and who the molecules bind to the protein. Importantly, we want to learn the Structure Activity Relationship from the model and try to propose new compounds given the ones we already know.\n",
    "\n",
    "First, load the data from the previous notebook from the `.csv` save on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb643786-a950-41f0-8908-c8b3cd0c9a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output_df = pd.read_csv(\"./binders.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0680850a-1872-4b08-b0cc-5070b4aa80a0",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We will first generate features (i.e. the independant variables in our dataset). In the previous step, we generated features according to Ro5. However this time, want to generate lots of additional features and see if they contain any additional information that will be useful for us further down the road.\n",
    "\n",
    "First things first, generate the features and try to understand them. Use `print(list(rich_descriptors.keys()))` to get the full list of descriptors.\n",
    "\n",
    "1.) In the cell below, we are also filtering the data to exclude features that have zero variance. Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e01fd-6df0-4099-bbc0-5266c81b8133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "\n",
    "rich_descriptors = output_df[\"smiles\"].apply(helpers.calculate_all_features)\n",
    "### Filter out features with zero variance\n",
    "variance = rich_descriptors.var()\n",
    "columns  = list()\n",
    "for c,v in variance.items():\n",
    "    if v > 0.:\n",
    "        columns.append(c)\n",
    "rich_descriptors = rich_descriptors[columns]\n",
    "data_rich_descriptors = output_df[[\"molecule_chembl_id\", \"pIC50\", \"IC50\"]].join(rich_descriptors)\n",
    "\n",
    "### These are the Ro5 features\n",
    "features_Ro5 = [\"ExactMolWt\", \"NumHAcceptors\", \"NumHDonors\", \"MolLogP\", \"NumRotatableBonds\"]\n",
    "### These are the advanced features\n",
    "features_advanced = features_Ro5 + [\n",
    "    \"MaxAbsEStateIndex\", \"MaxEStateIndex\", \"MinAbsEStateIndex\", \"MinEStateIndex\", \n",
    "    \"qed\", \"SPS\", \"MaxPartialCharge\", \"MinPartialCharge\", \"MaxAbsPartialCharge\", \n",
    "    \"MinAbsPartialCharge\", \"SlogP_VSA1\", \"SlogP_VSA2\", \"SlogP_VSA3\", \"SlogP_VSA4\", \n",
    "    \"SlogP_VSA5\", \"SlogP_VSA6\", \"SlogP_VSA7\", \"SlogP_VSA8\", \"SlogP_VSA10\", \"SlogP_VSA11\", \n",
    "    \"SlogP_VSA12\", \"PEOE_VSA1\", \"PEOE_VSA2\", \"PEOE_VSA3\", \"PEOE_VSA4\", \"PEOE_VSA5\", \n",
    "    \"PEOE_VSA6\", \"PEOE_VSA7\", \"PEOE_VSA8\", \"PEOE_VSA9\", \"PEOE_VSA10\", \"PEOE_VSA11\", \n",
    "    \"PEOE_VSA12\", \"PEOE_VSA13\", \"PEOE_VSA14\", \"SMR_VSA1\", \"SMR_VSA2\", \"SMR_VSA3\", \n",
    "    \"SMR_VSA4\", \"SMR_VSA5\", \"SMR_VSA6\", \"SMR_VSA7\", \"SMR_VSA9\", \"TPSA\", \"HallKierAlpha\", \n",
    "    \"Kappa1\", \"Kappa2\", \"Kappa3\", \"BertzCT\", \"AvgIpc\"\n",
    "]\n",
    "\n",
    "data_rich_descriptors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc241144-ce0d-43c6-a1b1-a3c07192c765",
   "metadata": {},
   "source": [
    "## Advanced descriptors\n",
    "\n",
    "In the following we will list advanced descriptors that are stored in the above dataframe.\n",
    "\n",
    "### Electrotopological state\n",
    "\n",
    "The electropological state (E-state) encodes the topology and electronic environment of each atom in a molecule. In the above dataframe, the features corresponding to the E-State are `MaxAbsEStateIndex`,  `MaxEStateIndex`, `MinAbsEStateIndex` and `MinEStateIndex`. Reference *J. Chem. Inf. Comput. Sci. 1991, 31, 76-82*\n",
    "\n",
    "### Drug-likliness\n",
    "\n",
    "The QED descriptor describes the drug-likeliness of a molecule. The namer of the descriptor in the above dataframe is `qed`. Reference: *Bickerton, G.R.; Paolini, G.V.; Besnard, J.; Muresan, S.; Hopkins, A.L. (2012) ‘Quantifying the chemical beauty of drugs’, Nature Chemistry, 4, 90-98*\n",
    "\n",
    "### Spatial complexity\n",
    "\n",
    "The Spatial complexity is described by the `SPS` descriptor. Reference *Krzyzanowski, A.; Pahl, A.; Grigalunas, M.; Waldmann, H. Spacial Score─A Comprehensive Topological Indicator for Small-Molecule Complexity. J. Med. Chem. 2023*\n",
    "\n",
    "### Partial Charges\n",
    "\n",
    "The Gasteiger partial charge of each atom in a molecule is physical parameter describing its static electric environment in a molecule. The corresponding decsriptors are `MaxPartialCharge`, `MinPartialCharge`, `MaxAbsPartialCharge` and `MinAbsPartialCharge`.\n",
    "\n",
    "### Lipophilicity\n",
    "\n",
    "The Lipophilicity of a molecule is described by a whole set of descriptors. In the above dataframe they are `SlogP_VSAX` (X=1-12). Reference: *Paul Labute, Journal of Molecular Graphics and Modelling 18, 464-477, 2000*. This is an interesting blog post: https://greglandrum.github.io/rdkit-blog/posts/2023-04-17-what-are-the-vsa-descriptors.html \n",
    "\n",
    "### Electrostatic interactions\n",
    "\n",
    "Electrostatic interactions descriptor. `PEOE_VSAX` (X=1-14). Same reference as above.\n",
    "\n",
    "### Molecular Polarazibility\n",
    "\n",
    "Molecular Polarazibility descriptor. `SMR_VSAX` (X=1-9). Same reference as above.\n",
    "\n",
    "### Polar Surface Area\n",
    "\n",
    "The total polar surface area is described by the `TPSA` descriptor. This descriptor describes the surface area in a given molecule that is attributed to polar atoms.\n",
    "\n",
    "### Topological Indices 1\n",
    "\n",
    "The topological indices developed by Hall and Kier describe the topology of the underlying molecule. In the above dataframe they are `HallKierAlpha`, `Kappa1`, `Kappa2` and `Kappa3`. Reference *Lowell H. Hall and Lemont B. Kier, Reviews in Computational Chemistry, Volume 2, 1991*\n",
    "\n",
    "### Topological Indices 2\n",
    "\n",
    "The `BertzCT` and `AvgIpc` descriptors quantify the topological properties of the underlying molecular graphs. The `BertzCT` quantifies molecule complexity of molecules. The `AvgIpc` qunatifies the information content contained in the adjacency matrix of the molecular graph. References *From S. H. Bertz, J. Am. Chem. Soc., vol 103, 3599-3601 (1981)* and *D. Bonchev & N. Trinajstic, J. Chem. Phys. vol 67, 4517-4533 (1977)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1cae9e-102b-464a-b01d-4c3cb38bb806",
   "metadata": {},
   "source": [
    "## Build a linear Multiple Least Squares Model\n",
    "\n",
    "### Test set size and model uncertainty\n",
    "\n",
    "Now, let us build a linear model using the features extraced in the previous step. Luckily we don't have to do that all by hand, but we can use existing model building architectures contained in the `scikit-learn` python package.\n",
    "We will first have to decide on the size of training set. In this case, we will use 25% of the data for testing. To evaluate the quality of our model, we will compute the coefficient of determination (R2) and the mean unsigned error (MUE). In addition we will report the 95% confidence interval obtained from `n_evaluations=600` repeated fits of our model. During each repitition a random split into test/training set was carried out. The results with CI are reported as **MEAN [LOWER_CI UPPER_CI]**.\n",
    "\n",
    "1.) What exactly is the different between test set and training set? Why do we need these two different sets of data?\n",
    "\n",
    "2.) How does the model perform with increasing/decreasing training/test set size?\n",
    "\n",
    "3.) What is a confidence interval and why is it useful? Tip: Look up the Central Limit Theorem.\n",
    "\n",
    "4.) How could you test whether the set of `R2` and `MUE` are actually Gaussian distributed?\n",
    "\n",
    "5.) Look at the linear coefficients. What do they tell you about the importance of the different features?\n",
    "\n",
    "6.) Repeat the analysis with the Ro5 features `features_Ro5`. How do you explain the decrease in performance? Note, only change the value of the `INPUT_FEATURES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f177544f-56f2-4015-bddb-bc81494b0973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "\n",
    "INPUT_FEATURES = features_advanced\n",
    "\n",
    "results, _ = helpers.fit_model(\n",
    "    model = linear_model.LinearRegression(), # model\n",
    "    n_evaluations = 1000, # number of attempts to generate a model\n",
    "    test_size = 0.10, # size of the test set\n",
    "    dataset = data_rich_descriptors, # full dataset\n",
    "    x_name = INPUT_FEATURES, # names of the descriptors (independant variable)\n",
    "    y_name = \"pIC50\", #name of dependent variable\n",
    "    properties = [\"coef_\"], # names of the properties to extract from model\n",
    ")\n",
    "\n",
    "R2_test       = results['R2_test'].mean()\n",
    "MUE_test      = results['MUE_test'].mean()\n",
    "CI95_R2_test  = helpers.get_CI(results['R2_test'])\n",
    "CI95_MUE_test = helpers.get_CI(results['MUE_test'])\n",
    "\n",
    "print(\n",
    "    f'Performance Statistics:', '\\n'\n",
    "    f'-----------------------', '\\n'\n",
    "    f'R2 test set : {R2_test:4.2f} [{CI95_R2_test[0]:4.2f} {CI95_R2_test[1]:4.2f}]', '\\n'\n",
    "    f'MUE test set: {MUE_test:4.2f} [{CI95_MUE_test[0]:4.2f} {CI95_MUE_test[1]:4.2f}]', '\\n'\n",
    ")\n",
    "print(\n",
    "    'Linear coefficients:','\\n'\n",
    "    '--------------------'\n",
    ")\n",
    "coef_ = np.array(results['coef_'])\n",
    "sorted_idxs = np.argsort(np.abs(coef_.mean(axis=0)))[::-1]\n",
    "for idx in sorted_idxs:\n",
    "    p = INPUT_FEATURES[idx]\n",
    "    v = coef_[:,idx].mean()\n",
    "    CI95 = helpers.get_CI(coef_[:,idx])\n",
    "    print(\n",
    "        f'{p:20s} : {v:4.6f} [{CI95[0]:4.6f} {CI95[1]:4.6f}]'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034a25be-0ea6-440d-96aa-3d993c41bdf4",
   "metadata": {},
   "source": [
    "### Model convergence\n",
    "\n",
    "Run the cell below to plot the average `R2` and `MUE` over the course of the fitting.\n",
    "\n",
    "1.) Explain the strong fluctuations in the beginning the fitting and why they become less later on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e57dd62-7562-4b78-94a3-a59f53ff6b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "data   = results[\"MUE_test\"]\n",
    "N      = data.size\n",
    "runavg = np.cumsum(data)/np.arange(1,N+1)\n",
    "axs[0].plot(runavg)\n",
    "axs[0].set_title(r\"$MUE$ test set\")\n",
    "axs[0].set_ylabel(\"MUE\")\n",
    "axs[0].set_xlabel(\"Iteration\")\n",
    "\n",
    "data   = results[\"R2_test\"]\n",
    "N      = data.size\n",
    "runavg = np.cumsum(data)/np.arange(1,N+1)\n",
    "axs[1].plot(runavg)\n",
    "axs[1].set_title(r\"$R^2$ test set\")\n",
    "axs[1].set_ylabel(r\"$R^2$\")\n",
    "axs[1].set_xlabel(\"Iteration\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8de374-97bf-4a49-bab2-eafc9cb49fe3",
   "metadata": {},
   "source": [
    "### Use other features and models\n",
    "\n",
    "In the next step, we will repeat our experiments from above but this time we will use a Random Forest regressor.\n",
    "\n",
    "1.) Read up on the random forest regressor and understand how it works. How is this regressor related to decision trees?\n",
    "\n",
    "2.) Repeat the model fitting with a random forest model. Below, the value of the model parameter of the function  `helpers.fit_model` was set to `ensemble.RandomForestRegressor()`.\n",
    "\n",
    "3.) Random Forests are able to rank the input features based on how much they contribute to the prediction. Look at the feature importance value reported below and explain which features seem to be more important than others. Also relate your findings to the analysis of the linear coefficients in the Linear Least Square Model above. \n",
    "\n",
    "4.) Repeat the analysis with the Ro5 features `features_Ro5`. Explain the observed difference.\n",
    "\n",
    "Note: The cell below will save the best model that was found over `n_evaluations`. The best model will be stored in the variable `reg_model`. Keep this in mind when running the last cell of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c4f8b8-96e3-4718-93bd-2bf352c18729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import dummy\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "\n",
    "INPUT_FEATURES = features_advanced\n",
    "\n",
    "results, reg_model = helpers.fit_model(\n",
    "    model = ensemble.RandomForestRegressor(), # model\n",
    "    n_evaluations = 100, # number of attempts to generate a model\n",
    "    test_size = 0.10, # size of the test set\n",
    "    dataset = data_rich_descriptors, # full dataset\n",
    "    x_name = INPUT_FEATURES, # names of the descriptors (independant variable)\n",
    "    y_name = \"pIC50\", #name of dependent variable\n",
    "    properties = [\"feature_importances_\"], # names of the properties to extract from model\n",
    ")\n",
    "\n",
    "R2_test  = results['R2_test'].mean()\n",
    "MUE_test = results['MUE_test'].mean()\n",
    "CI95_R2_test = helpers.get_CI(results['R2_test'])\n",
    "CI95_MUE_test = helpers.get_CI(results['MUE_test'])\n",
    "\n",
    "print(\n",
    "    f'Performance Statistics:', '\\n'\n",
    "    f'-----------------------', '\\n'\n",
    "    f'R2 test set : {R2_test:4.2f} [{CI95_R2_test[0]:4.2f} {CI95_R2_test[1]:4.2f}]', '\\n'\n",
    "    f'MUE test set: {MUE_test:4.2f} [{CI95_MUE_test[0]:4.2f} {CI95_MUE_test[1]:4.2f}]', '\\n'\n",
    ")\n",
    "\n",
    "print(\n",
    "    'Feature importance:','\\n'\n",
    "    '-------------------'\n",
    ")\n",
    "coef_ = np.array(results['feature_importances_'])\n",
    "sorted_idxs = np.argsort(coef_.mean(axis=0))[::-1]\n",
    "for idx in sorted_idxs:\n",
    "    p = INPUT_FEATURES[idx]\n",
    "    v = coef_[:,idx].mean()\n",
    "    CI95 = helpers.get_CI(coef_[:,idx])\n",
    "    print(\n",
    "        f'{p:20s} : {v:4.6f} [{CI95[0]:4.6f} {CI95[1]:4.6f}]'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3859c38b-0ced-4a3b-820c-59ef64cde2a0",
   "metadata": {},
   "source": [
    "## Testing new molecules\n",
    "\n",
    "Imagine that now after building a nice predictive model, you are asked by a Medicinal Chemist to provide a list of four molecules that will bind to our target protein. They provide you with an initial list of eight molecules and you are asked to tell them the ones that are most likely to bind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c7dfbf-c300-47ff-afbc-a1c4eb7fa82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import Descriptors, Draw, PandasTools\n",
    "import pandas as pd\n",
    "\n",
    "test_mols = [\n",
    "    \"O=S(C2=CC=CC1=CN=CC=C12)(N3CCNC[C@@H]3C)=O\",\n",
    "    \"O=S(C2=CC=CC1=CN=CC=C12)(N3CCCNCC3)=O\",\n",
    "    \"O=S(C2=CC=CC1=CN=CC=C12)(N(CCC)CCN)=O\",\n",
    "    \"O=S(C2=CC=CC1=CN=CC=C12)(NCCNCCC)=O\",\n",
    "    \"O=S(C2=CC=CC1=CN=CC=C12)(NCCNC(CCC)CC)=O\",\n",
    "    \"O=S(C2=CC=CC1=CN=CC=C12)(NC(CCSCNO)CNCCC)=O\",\n",
    "    \"O=C(C2=CC=CC1=CN=CC=C12)(NCCNCCC)\",\n",
    "    \"O=S(C2=CC=CC1=CC=CC=C12)(N3CCCNCC3)=O\"\n",
    "]\n",
    "\n",
    "molecules = pd.DataFrame(\n",
    "    {\"smiles\" : pd.Series(test_mols,                          \n",
    "                         )\n",
    "    }\n",
    ")\n",
    "PandasTools.RenderImagesInAllDataFrames(images=True)\n",
    "PandasTools.AddMoleculeColumnToFrame(molecules, \"smiles\", includeFingerprints=True)\n",
    "molecules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd8e379-d49e-4452-aa71-898516fb89d6",
   "metadata": {},
   "source": [
    "**Which molecules will you select based on your model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f484b7b5-7dec-4a60-b49b-d672afcfe734",
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_descriptors = molecules[\"smiles\"].apply(helpers.calculate_all_features)\n",
    "reg_model.predict(rich_descriptors[INPUT_FEATURES])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8018c8f3-08d1-424c-a667-136418a5cb46",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Great, you've worked your way all through the notebook. Make sure that you have a good answer for each of the questions above as they will guide you through the report."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
